{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4bd820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from articles import yield_wiki_articles\n",
    "from parse_wiki_markup import parse_wiki_markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed07910",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_FILE = r\"C:\\\\Users\\\\Lenovo\\\\Downloads\\\\wikidump\\\\enwiki-20250501-pages-articles-multistream-index.txt.bz2\"\n",
    "BIG_FILE = r\"C:\\\\Users\\\\Lenovo\\\\Downloads\\\\wikidump\\\\enwiki-20250501-pages-articles.xml.bz2\"\n",
    "\n",
    "articles = yield_wiki_articles(BIG_FILE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96364c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = next(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f34348",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = parse_wiki_markup(article['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98cad57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "# --- SETUP ---\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --- OLLAMA BRIDGE ---\n",
    "class OllamaVerifier:\n",
    "    def __init__(self, model=\"llama3\"):\n",
    "        self.api_url = \"http://localhost:11434/api/generate\"\n",
    "        self.model = model\n",
    "\n",
    "    def verify_is_person(self, name, context_snippet):\n",
    "        \"\"\"\n",
    "        Returns True (1) if LLM thinks it's a person, False (0) otherwise.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            f\"Analyze the text snippet below.\\n\"\n",
    "            f\"Text: \\\"{context_snippet}\\\"\\n\"\n",
    "            f\"Question: Is the entity '{name}' strictly a human being in this specific context? \"\n",
    "            f\"Ignore statues, tombs, wars, or organizations.\\n\"\n",
    "            f\"Reply with exactly one digit: 1 for Yes, 0 for No.\"\n",
    "        )\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": 0} # Deterministic\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(self.api_url, json=payload, timeout=2)\n",
    "            result = response.json().get('response', '').strip()\n",
    "            # Parse for 1 or 0\n",
    "            if '1' in result: return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            # If Ollama fails/times out, default to keeping it (fail open) \n",
    "            # or discarding it (fail closed). Here we fail closed to be safe.\n",
    "            print(f\"âš ï¸ Ollama Error: {e}\")\n",
    "            return False\n",
    "\n",
    "# --- EXTRACTOR ---\n",
    "class BaseWikiExtractor:\n",
    "    \n",
    "    RE_YEAR = re.compile(r'\\b(1\\d{3}|20[0-2]\\d)\\b')\n",
    "\n",
    "    MODEL=\"llama3:latest\"\n",
    "    \n",
    "    # Heuristics for \"High Probability\" vs \"Low Probability\"\n",
    "    # If a name contains these, it's \"Low Prob\" and needs LLM verification\n",
    "    RISKY_KEYWORDS = {\n",
    "        \"king\", \"queen\", \"prince\", \"princess\", \"pope\", \"saint\", \"st.\", \"baron\", \"lord\", # Titles\n",
    "        \"the\", \"of\", \"de\", \"van\", \"von\", \"bin\", \"al\", # Particles (often messy)\n",
    "        \"wrath\", \"treaty\", \"war\", \"battle\", \"tomb\", \"statue\", \"memorial\", # Objects/Events\n",
    "        \"v\", \"i\", \"ii\", \"iii\", \"iv\", \"x\", \"vi\", # Roman numerals (e.g. Nicholas V)\n",
    "        \"great\", \"terrible\", \"bold\", \"lion\" # Epithets\n",
    "    }\n",
    "\n",
    "    def __init__(self, model=MODEL):\n",
    "        self.ollama = OllamaVerifier(model=model) # Change model as needed\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        if not isinstance(text, str): return \"\"\n",
    "        text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n",
    "        text = re.sub(r'\\[\\[(File|Image|Category):.*?\\]\\]', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[\\[(?:[^|\\]]*\\|)?([^\\]]+)\\]\\]', r'\\1', text)\n",
    "        text = re.sub(r'==+.*?==+', '', text)\n",
    "        text = re.sub(r'\\w+=\"[^\"]+\"', '', text) \n",
    "        text = re.sub(r\"'{2,}\", \"\", text) \n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def _is_suspicious_name(self, text):\n",
    "        \"\"\"\n",
    "        Dual Channel Logic:\n",
    "        Returns TRUE if the name is 'Risky' (Channel 2).\n",
    "        Returns FALSE if the name is 'High Probability' (Channel 1).\n",
    "        \"\"\"\n",
    "        lower = text.lower()\n",
    "        tokens = set(re.split(r'\\W+', lower))\n",
    "        \n",
    "        # 1. Check for Risky Keywords (Titles, Particles, Epithets)\n",
    "        if not tokens.isdisjoint(self.RISKY_KEYWORDS):\n",
    "            return True\n",
    "            \n",
    "        # 2. Check for Roman Numerals at the end (e.g. \"Nicholas V\")\n",
    "        if re.search(r'\\b[IVX]+\\b$', text):\n",
    "            return True\n",
    "            \n",
    "        # 3. Check for Non-Title Case (e.g. \"tÃºmulo de...\")\n",
    "        # A valid person name in English Wiki is almost always Title Cased.\n",
    "        if not text[0].isupper():\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    def _find_triplets_in_text(self, text):\n",
    "        if len(text) > 50000: text = text[:50000]\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        triplets = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            persons = []\n",
    "            locations = []\n",
    "            years = []\n",
    "\n",
    "            # 1. NER Extraction\n",
    "            for ent in sent.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    # Basic cleanup\n",
    "                    clean_name = ent.text.strip()\n",
    "                    if len(clean_name) > 3 and len(clean_name.split()) >= 2:\n",
    "                        persons.append((clean_name, ent.start, ent.end))\n",
    "                \n",
    "                elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "                    if len(ent.text) > 2 and not re.search(r'\\d', ent.text):\n",
    "                        locations.append((ent.text, ent.start))\n",
    "                \n",
    "                elif ent.label_ == \"DATE\":\n",
    "                    match = self.RE_YEAR.search(ent.text)\n",
    "                    if match:\n",
    "                        years.append((match.group(1), ent.start))\n",
    "\n",
    "            # 2. Logic & Verification\n",
    "            if persons and locations and years:\n",
    "                for p_text, p_start, p_end in persons:\n",
    "                    \n",
    "                    # --- CHANNEL SELECTION ---\n",
    "                    is_valid_person = True\n",
    "                    \n",
    "                    if self._is_suspicious_name(p_text):\n",
    "                        # CHANNEL 2: LOW PROBABILITY -> LLM VERIFICATION\n",
    "                        # Extract context snippet (the full sentence)\n",
    "                        context = sent.text\n",
    "                        # Call Ollama (returns 0 or 1)\n",
    "                        # print(f\"ðŸ” Verifying suspicious entity: '{p_text}'...\")\n",
    "                        if not self.ollama.verify_is_person(p_text, context):\n",
    "                            is_valid_person = False\n",
    "                    \n",
    "                    # CHANNEL 1: High Prob -> Accepted automatically\n",
    "                    \n",
    "                    if is_valid_person:\n",
    "                        # Proximity Logic\n",
    "                        best_loc = min(locations, key=lambda x: abs(x[1] - p_start))\n",
    "                        best_year = min(years, key=lambda x: abs(x[1] - p_start))\n",
    "                        \n",
    "                        if abs(best_loc[1] - p_start) < 25 and abs(best_year[1] - p_start) < 25:\n",
    "                            triplets.append((p_text, best_loc[0], best_year[0]))\n",
    "\n",
    "        return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603e27f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m     article = \u001b[38;5;28mnext\u001b[39m(articles)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     parts = \u001b[43mparse_wiki_markup\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     results = extract_all_triplets(parts)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\projects\\24nme\\engine\\parse_wiki_markup.py:47\u001b[39m, in \u001b[36mparse_wiki_markup\u001b[39m\u001b[34m(wiki_text)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m parsed.get_lists():\n\u001b[32m     45\u001b[39m     l.string = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m raw_text = \u001b[43mparsed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplain_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtables\u001b[39m\u001b[33m\"\u001b[39m: tables,\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbullets\u001b[39m\u001b[33m\"\u001b[39m: bullet_groups,\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minfoboxes\u001b[39m\u001b[33m\"\u001b[39m: infoboxes,\n\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mraw_text\u001b[39m\u001b[33m\"\u001b[39m: raw_text\n\u001b[32m     54\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\projects\\24nme\\.venv\\Lib\\site-packages\\wikitextparser\\_wikitext.py:685\u001b[39m, in \u001b[36mWikiText.plain_text\u001b[39m\u001b[34m(self, replace_templates, replace_parser_functions, replace_parameters, replace_tags, replace_external_links, replace_wikilinks, unescape_html_entities, replace_bolds_and_italics, replace_tables, _is_root_node)\u001b[39m\n\u001b[32m    682\u001b[39m         remove(b, e)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m replace_external_links:\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparsed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexternal_links\u001b[49m:\n\u001b[32m    686\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m el.in_brackets:\n\u001b[32m    687\u001b[39m             b, e = el.span\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\projects\\24nme\\.venv\\Lib\\site-packages\\wikitextparser\\_wikitext.py:1255\u001b[39m, in \u001b[36mWikiText.external_links\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1253\u001b[39m spans = type_to_spans.setdefault(\u001b[33m'\u001b[39m\u001b[33mExternalLink\u001b[39m\u001b[33m'\u001b[39m, [])\n\u001b[32m   1254\u001b[39m span_tuple_to_span_get = {(s[\u001b[32m0\u001b[39m], s[\u001b[32m1\u001b[39m]): s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m spans}.get\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m el_shadow = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ext_link_shadow\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_extract\u001b[39m(start, end):\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m EXTERNAL_LINK_FINDITER(el_shadow, start, end):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\projects\\24nme\\.venv\\Lib\\site-packages\\wikitextparser\\_wikitext.py:1225\u001b[39m, in \u001b[36mWikiText._ext_link_shadow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1223\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m type_ \u001b[38;5;129;01min\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mTemplate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mParserFunction\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mParameter\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   1224\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s, e, _, _ \u001b[38;5;129;01min\u001b[39;00m subspans(type_):\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m         byte_array[s - ss : e - ss] = INVALID_EL_TPP_CHRS_SUB(\n\u001b[32m   1226\u001b[39m             \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, byte_array[s:e]\n\u001b[32m   1227\u001b[39m         )\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m byte_array\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "unique_results = set()\n",
    "found = 0\n",
    "while True:\n",
    "    try:\n",
    "        article = next(articles)\n",
    "        parts = parse_wiki_markup(article['text'])\n",
    "        results = extract_all_triplets(parts)\n",
    "        if results:\n",
    "            print(results)\n",
    "            break\n",
    "            found += len(results)\n",
    "            #print(f\"\\nFound {len(results)} triplets:\")\n",
    "            for r in results:\n",
    "                try:\n",
    "                    unique_results.add(r)\n",
    "                    found += 1\n",
    "                except Exception as ee:\n",
    "                    continue\n",
    "        print(f\"Total triplets found: {found}\", end='\\r')\n",
    "    except Exception as ae:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68aeaf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unique_results)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85887e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Category:20th-century'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m [a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(unique_results) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mMark\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(a) \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m1650\u001b[39m > \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m < \u001b[32m2025\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: 'Category:20th-century'"
     ]
    }
   ],
   "source": [
    "TARGET='Thopmson'\n",
    "MIN_YEAR = 1650\n",
    "MAX_YEAR = 1720\n",
    "for a in list(unique_results): \n",
    "    try:\n",
    "        year = int(a[2])\n",
    "        if 1650 > year < 2025:\n",
    "            if TARGET in str(a):\n",
    "                print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50ce53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gliner'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgliner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GLiNER\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load a small, fast model (runs on CPU easily)\u001b[39;00m\n\u001b[32m      4\u001b[39m model = GLiNER.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33murchade/gliner_small-v2.1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gliner'"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "# Load a small, fast model (runs on CPU easily)\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_small-v2.1\")\n",
    "\n",
    "text = \"While visiting the Metro in 2024, Philip Alston met with officials.\"\n",
    "labels = [\"person\", \"location\", \"year\"] # You define these on the fly!\n",
    "\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(f\"{entity['text']} => {entity['label']}\")\n",
    "    # Output:\n",
    "    # Philip Alston => person\n",
    "    # Metro => location\n",
    "    # 2024 => year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd3cb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'instructor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minstructor\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'instructor'"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Define the EXACT structure you want\n",
    "class BioEvent(BaseModel):\n",
    "    person: str\n",
    "    location: str\n",
    "    year: int\n",
    "\n",
    "class ExtractionResult(BaseModel):\n",
    "    events: List[BioEvent]\n",
    "\n",
    "# Patch the client (works with Ollama too!)\n",
    "client = instructor.patch(OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"))\n",
    "\n",
    "def extract_with_llm(text):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"llama3:latest\"\n",
    "    ,\n",
    "        response_model=ExtractionResult,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Extract biographical events from: {text}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Result is a guaranteed valid python object, no regex needed.\n",
    "# result.events[0].person == \"Philip Alston\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b6816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
