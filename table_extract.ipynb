{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5822c8b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated f-string literal (detected at line 53) (parse_wiki_markup.py, line 53)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92mc:\\Users\\Lenovo\\Documents\\projects\\24nme\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[36m  \u001b[39m\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom engine.parse_wiki_markup import wikimarkup_to_html\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\Documents\\projects\\24nme\\engine\\parse_wiki_markup.py:53\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"Warning:\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated f-string literal (detected at line 53)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from engine.parse_wiki_markup import wikimarkup_to_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.llm import clean_json_response, call_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8e4b063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "# line_profiler injects '@profile' into the built-ins when run via kernprof\n",
    "# If running normally, we need a dummy decorator to avoid NameError\n",
    "try:\n",
    "    @profile\n",
    "    def func(): pass\n",
    "except NameError:\n",
    "    def profile(func): return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# --- PROFILER SAFETY SHIM ---\n",
    "# This allows the code to run even if you aren't using kernprof/line_profiler\n",
    "try:\n",
    "    @profile\n",
    "    def _dummy(): pass\n",
    "except NameError:\n",
    "    def profile(func): return func\n",
    "# ----------------------------\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_llm_json(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strips markdown code blocks (```json ... ```) often added by LLMs.\n",
    "    Returns the raw JSON string.\n",
    "    \"\"\"\n",
    "    # specific regex to capture content inside ```json ... ``` or just ``` ... ```\n",
    "    match = re.search(r\"```(?:json)?\\s*(.*)\\s*```\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text.strip()\n",
    "\n",
    "def call_ollama(system_prompt: str, user_content: str, model: str = \"llama3\") -> str:\n",
    "    \"\"\"\n",
    "    A real, fully fleshed-out call to a local Ollama instance.\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"format\": \"json\",  # CRITICAL: Forces the model to output valid JSON structure\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,  # Low temp = more deterministic/factual\n",
    "            \"num_ctx\": 4096      # Context window size\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        raw_content = result['message']['content']\n",
    "        \n",
    "        # Even with \"format\": \"json\", models sometimes wrap it in markdown\n",
    "        return clean_llm_json(raw_content)\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"❌ Error: Could not connect to Ollama. Is it running? (http://localhost:11434)\")\n",
    "        return \"{}\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ LLM Error: {e}\")\n",
    "        return \"{}\"\n",
    "\n",
    "# --- HOW TO USE ---\n",
    "# extractor = WikiTableExtractor(call_ollama)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ca56361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine.articles import yield_wiki_articles\n",
    "from engine.parse_wiki_markup import parse_wiki_markup\n",
    "\n",
    "\n",
    "def all_articles():\n",
    "    BIG_FILE = r\"C:\\\\Users\\\\Lenovo\\\\Downloads\\\\wikidump\\\\enwiki-20250501-pages-articles.xml.bz2\"\n",
    "    return yield_wiki_articles(BIG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from pandas import DataFrame\n",
    "from engine.extractor import WikiTableExtractor\n",
    "# Assuming these exist in your scope from previous steps:\n",
    "extractor = WikiTableExtractor(call_ollama)\n",
    "\n",
    "\n",
    "def has_rows(df: DataFrame) -> bool:\n",
    "    return not df.empty\n",
    "\n",
    "@profile\n",
    "def person_location_year_df() -> Iterator[DataFrame]:\n",
    "    tables_found = 0\n",
    "    tables_attempted = 0\n",
    "\n",
    "    # 1. Loop through the generator, don't just call next() once\n",
    "    for article in all_articles():\n",
    "        \n",
    "        # Guard clause in case article format varies\n",
    "        if 'text' not in article:\n",
    "            continue\n",
    "\n",
    "        parts = parse_wiki_markup(article['text'])\n",
    "        \n",
    "        # Safely get tables, defaulting to empty list if key missing\n",
    "        tables = parts.get('tables', []) \n",
    "\n",
    "        for table in tables:\n",
    "            tables_attempted += 1\n",
    "            \n",
    "            # Optional: Print status every 10 tables to reduce console noise\n",
    "            if tables_attempted % 10 == 0:\n",
    "                print(f\"Attempts: {tables_attempted} | Found: {tables_found}\")\n",
    "\n",
    "            try:\n",
    "                # 2. Conversion Pipeline\n",
    "                html_table = wikimarkup_to_html(table)\n",
    "                \n",
    "                # 3. LLM + Pandas Extraction\n",
    "                # Note: process_page_html returns an empty DF if no people found\n",
    "                df = extractor.process_page_html(html_table)\n",
    "\n",
    "                if has_rows(df):\n",
    "                    tables_found += 1\n",
    "                    yield df\n",
    "\n",
    "            except Exception as e:\n",
    "                # 4. Error Handling\n",
    "                # If a specific table causes a crash (e.g., malformed HTML), \n",
    "                # log it and continue to the next table/article.\n",
    "                print(f\"Error processing table index {tables_attempted}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e92783b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function patched successfully!\n"
     ]
    }
   ],
   "source": [
    "import wikitextparser as wtp\n",
    "\n",
    "# We redefine this function explicitly in the notebook to override the external file\n",
    "def parse_wiki_markup(wiki_text):\n",
    "    parsed = wtp.parse(wiki_text)\n",
    "    \n",
    "    # Extract tables\n",
    "    tables = [t.string for t in parsed.tables]\n",
    "    \n",
    "    # Extract bullet lists\n",
    "    bullet_groups = []\n",
    "    for l in parsed.get_lists():\n",
    "        bullet_groups.append(l.items)\n",
    "        \n",
    "    # Extract infoboxes\n",
    "    infoboxes = []\n",
    "    for t in parsed.templates:\n",
    "        if \"infobox\" in t.name.lower():\n",
    "            infoboxes.append(t.string)\n",
    "\n",
    "    # Clean lists from text to avoid duplication\n",
    "    for l in parsed.get_lists():\n",
    "        l.string = \"\"\n",
    "\n",
    "    # --- THE FIX IS HERE ---\n",
    "    try:\n",
    "        raw_text = parsed.plain_text().strip()\n",
    "    except (AttributeError, Exception):\n",
    "        # If wikitextparser crashes, return empty string so pipeline survives\n",
    "        raw_text = \"\"\n",
    "    # -----------------------\n",
    "\n",
    "    return {\n",
    "        \"tables\": tables,\n",
    "        \"bullets\": bullet_groups,\n",
    "        \"infoboxes\": infoboxes,\n",
    "        \"raw_text\": raw_text\n",
    "    }\n",
    "\n",
    "print(\"Function patched successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bda9f925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempts: 10 | Found: 0\n",
      "Attempts: 20 | Found: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 601.919 s\n",
      "File: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_7588\\2204833369.py\n",
      "Function: step at line 5\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     5                                           def step():\n",
      "     6         1         47.0     47.0      0.0      try:\n",
      "     7         1 6019190539.0 6.02e+09    100.0          return next(dataframes)\n",
      "     8                                               except StopIteration:\n",
      "     9                                                   return None\n",
      "\n",
      "Total time: 597.316 s\n",
      "File: C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_7588\\4033124426.py\n",
      "Function: WikiTableExtractor.process_page_html at line 127\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   127                                               @profile  # <--- PROFILING POINT 3: Measures Pandas operations vs LLM wait time\n",
      "   128                                               def process_page_html(self, html_content: str) -> pd.DataFrame:\n",
      "   129                                                   \"\"\"\n",
      "   130                                                   Main pipeline logic.\n",
      "   131                                                   \"\"\"\n",
      "   132        21        346.0     16.5      0.0          try:\n",
      "   133                                                       # Profiler will show if reading HTML is a bottleneck\n",
      "   134        21    2242437.0 106782.7      0.0              tables = pd.read_html(StringIO(html_content), keep_default_na=False)\n",
      "   135                                                   except ValueError:\n",
      "   136                                                       return pd.DataFrame() \n",
      "   137                                           \n",
      "   138        21        276.0     13.1      0.0          all_extracted_people = []\n",
      "   139                                           \n",
      "   140        42       1168.0     27.8      0.0          for i, df in enumerate(tables):\n",
      "   141        21     679229.0  32344.2      0.0              df = df.dropna(how='all')\n",
      "   142        21        486.0     23.1      0.0              caption = f\"Table {i}\" \n",
      "   143                                           \n",
      "   144                                                       # --- LLM STEP 1 & 2 ---\n",
      "   145        21      66611.0   3172.0      0.0              context = self._get_table_context(df, caption)\n",
      "   146        21 5766725192.0 2.75e+08     96.5              analysis = self.analyze_table_metadata(context) # This line will likely show high % time\n",
      "   147                                           \n",
      "   148        21        545.0     26.0      0.0              if not analysis or not analysis.get(\"is_people_table\"):\n",
      "   149        19        229.0     12.1      0.0                  continue\n",
      "   150                                           \n",
      "   151         2         49.0     24.5      0.0              mappings = analysis.get(\"mappings\", {})\n",
      "   152         2         27.0     13.5      0.0              col_name = mappings.get(\"person_name\")\n",
      "   153         2         27.0     13.5      0.0              col_loc = mappings.get(\"location\")\n",
      "   154         2         24.0     12.0      0.0              col_year = mappings.get(\"year\")\n",
      "   155                                           \n",
      "   156         2         22.0     11.0      0.0              if not col_name:\n",
      "   157                                                           continue\n",
      "   158                                           \n",
      "   159                                                       # --- PANDAS STEP 3 ---\n",
      "   160         2      35003.0  17501.5      0.0              temp_df = pd.DataFrame()\n",
      "   161                                           \n",
      "   162                                                       # Profiler will confirm these operations are near-instant\n",
      "   163         2      13544.0   6772.0      0.0              if col_name in df.columns:\n",
      "   164         1     274273.0 274273.0      0.0                  temp_df['person_name'] = df[col_name]\n",
      "   165                                                       else:\n",
      "   166         1         17.0     17.0      0.0                  continue \n",
      "   167                                           \n",
      "   168         1        722.0    722.0      0.0              if col_loc in df.columns:\n",
      "   169                                                           temp_df['location'] = df[col_loc]\n",
      "   170                                                       else:\n",
      "   171         1      30811.0  30811.0      0.0                  temp_df['location'] = col_loc \n",
      "   172                                           \n",
      "   173         1        522.0    522.0      0.0              if col_year in df.columns:\n",
      "   174                                                           temp_df['year'] = df[col_year]\n",
      "   175                                                       else:\n",
      "   176         1      13497.0  13497.0      0.0                  temp_df['year'] = col_year\n",
      "   177                                           \n",
      "   178                                                       # --- LLM STEP 4 ---\n",
      "   179         1         76.0     76.0      0.0              if len(temp_df) > 0:\n",
      "   180         1     516891.0 516891.0      0.0                  sample = temp_df.head(3).to_dict(orient='records')\n",
      "   181                                                           # This line will show the verification cost\n",
      "   182         1  201769601.0 2.02e+08      3.4                  if self.verify_extraction(sample):\n",
      "   183         1         24.0     24.0      0.0                      all_extracted_people.append(temp_df)\n",
      "   184                                           \n",
      "   185        21        276.0     13.1      0.0          if all_extracted_people:\n",
      "   186         1      11732.0  11732.0      0.0              return pd.concat(all_extracted_people, ignore_index=True)\n",
      "   187        20     780357.0  39017.8      0.0          return pd.DataFrame(columns=['person_name', 'location', 'year'])"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the generator\n",
    "dataframes = person_location_year_df()\n",
    "\n",
    "# 2. Define a helper to profile a single step\n",
    "def step():\n",
    "    try:\n",
    "        return next(dataframes)\n",
    "    except StopIteration:\n",
    "        return None\n",
    "\n",
    "# 3. Run the profiler on the helper\n",
    "%lprun -f extractor.process_page_html -f step step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c25ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6f8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
